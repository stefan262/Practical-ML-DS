{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Prerequisites\n",
    "- [Classification](https://github.com/AI-Core/Strong-ML-Foundations/blob/master/Classification.ipynb)\n",
    "\n",
    "\n",
    "## Why do simple models struggle with meaningful tasks?\n",
    "\n",
    "Whereas the size of a house and its price might be linearly correlated, the pixel intensities of an image are certainly not linearly correlated with whether it contains a dog or a cat.\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We need to build much more complex models to solve harder problems.\n",
    "\n",
    "## Can we build more complex models by combining many simple transformations?\n",
    "\n",
    "The models we have just seen apply a single transformation to the data. However most problems of practical interest can't be solved using such simple models. \n",
    "\n",
    "Models with greater **capacity** are those which are able to model more complicated functions.\n",
    "\n",
    "A single linear transformation (multiplication by weights of model) stretches the input space by a certain factor in some direction, and adding a constant (bias) shifts it. \n",
    "We call models which apply  \n",
    "What if we applied more than one **layer** of transformations to our inputs, to create a **deep model**. Would we be able to increase the capacity of our model and make it able to model more complex input-output relationships, particularly non-linear ones?\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "...well, not quite yet.\n",
    "\n",
    "...if we repeatedly apply a linear transformation, the input can be factorised out of the output, showing that many repeated linear functions are eventually equal to a single linear transformation.\n",
    "\n",
    "![](./images/factor-proof.png)\n",
    "\n",
    "## So how can we increase the capacity of our models?\n",
    "\n",
    "We want to be able to model non-linear functions, so let's try to throw in some non-linear transformations into our model.\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "These non-linear functions prevent the input being able to be factorised out of the model. Hence the overall transformation can represent non-linear input-output relationships.\n",
    "\n",
    "We call these non-linear functions **activation functions**.\n",
    "\n",
    "However, It's not like we want to introduce really complicated functions into our model - ideally we wouldn't even have to and we could keep things simple. So let's try and complicate things only a minimal amount by keeping our activation functions very simple.\n",
    "\n",
    "Here are some common activation functions. ReLU (Rectified Linear Unit) is by far the most widely used.\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "Now we have all of the ingredients to fully understand how we can model more complicated functions. Let's look at that all together:\n",
    "\n",
    "![](./images/full-nn.png)\n",
    "\n",
    "Guess what? That is a **neural network**. Surprise.\n",
    "\n",
    "It's just repeated simple linear transformations followed by simple non-linear transformations (activation functions). Simple.\n",
    "\n",
    "Let's learn some jargon.\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "Neural networks have additional hyperparameters of the depth of the model and the width of each layer. These \n",
    "\n",
    "## What can neural networks do?\n",
    "\n",
    "The motivation that led us to deriving neural networks was that we wanted to model more complex functions. But what functions can a neural network actually represent? Well, as we show below they can actually represent almost all continuous functions. Neural Networks are **general function approximators**.\n",
    "\n",
    "![](./images/univ-approx.png)\n",
    "\n",
    "## How can our neural networks learn to model some function?\n",
    "\n",
    "As we did in the optimisation notebook, we can adjust our model parameters using gradient descent as such:\n",
    "1. Pass input data forward through model to output a prediction\n",
    "2. Calculate loss between predicted output and output label\n",
    "3. Find direction that moving the model parameters in will reduce the error\n",
    "4. Move model weights (parameters) a small amount in that direction \n",
    "\n",
    "![](./images/backprop.png)\n",
    "\n",
    "Here you can see that many terms reappear when computing the gradients of preceeding layers. \n",
    "By caching those terms, we save having to recompute them for these layers nearer the input. This makes finding the gradients of the loss with respect to each weight in the model much more efficient both in terms of memory and speed. \n",
    "This process of computing these gradients effectively is called the **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare our data\n",
    "\n",
    "Today we are going to look at a dataset called MNIST (em-nist). It consists of 70,000 images of hand drawn digits from 0-9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-09-30T20:15:28.797189</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p63a874339f)\">\n    <image height=\"218\" id=\"imagefc4335d924\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAGfklEQVR4nO3dTYjNbQPH8fHMLTQ2ygJ5jWI1K8nGYsbCYsiUSN5KsmIxFpIaL8PEAhubaYRsLCRioywY0SgbpdTshpJISc1QJO5n9dz1dN/nOvfMnPM7Y3w+21//c67Nt6vm38xMa2pq+rMJqKv/NPoA8DsQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQcAfjT5Ao8yePbu4b9q0qbgvXbq0uPf29o71SH+5dOlScX/8+PG4P7upqalpeHi44vbs2bMJfTb/zI0GAUKDAKFBgNAgQGgQIDQIEBoETGtqavqz0Yeoh40bNxb3rq6u4t7W1lbD00wub968qbht3769+OzQ0FCtj/OX0dHR4v7z58+6fXe9udEgQGgQIDQIEBoECA0ChAYBQoOAX/o92owZMypu165dKz67bdu2Gp+GiTpx4kRx//jxY3G/evVqcf/27duYz1QrbjQIEBoECA0ChAYBQoMAoUGA0CBgUr9Hq/a3F8+fP19x279/f62PMyafPn2quI2MjEzosxcvXlzcp02bNqHP/1V1d3cX97Nnz4ZO8nduNAgQGgQIDQKEBgFCgwChQcCk/rdNK1asKO7t7e2hk/xdf39/cb9+/XrFbXBwcELfffLkyeLe3Nw87s/euXNncV+yZMm4P7ve3r592+gjVORGgwChQYDQIEBoECA0CBAaBAgNAib1r8lUs2rVqorb/fv3i88uXLhwQt/94MGD4r5r166K24cPHyb03fVU7T1ZS0tLce/t7Z3Q50/E58+fi/vRo0eL+9OnT2t5nP/jRoMAoUGA0CBAaBAgNAgQGgQIDQJ+6fdoJQcOHCjuFy9erOv3P3r0qOLW2dlZfHZ0dLS2h6Hh3GgQIDQIEBoECA0ChAYBQoMAoUHAlH2PVu33pqr9zchbt24V92XLlo35TP8zMDBQ3Hfv3l3c3717N+7vpjHcaBAgNAgQGgQIDQKEBgFCg4Ap++P9ieru7i7uPT09dfvuJ0+eFPcbN24U976+vloehxpwo0GA0CBAaBAgNAgQGgQIDQKEBgHeo1Uwffr04r58+fLifvfu3YpbtV/RqebHjx/F/fv37+P+7OPHjxf3CxcujPuzf2duNAgQGgQIDQKEBgFCgwChQYDQIMB7tDppbW2tuG3evLn47LFjx4p7c3PzuM70b3z9+rW437t3r7gfPny4uL9+/XqsR5oS3GgQIDQIEBoECA0ChAYBQoMAoUGA92iT0N69e4t7f39/ca/ne7ZqTp06Vdzv3LlTcXvx4kWNTzN5uNEgQGgQIDQIEBoECA0ChAYBQoMA79F+QW1tbcX90KFDxb2jo6OWxxmT4eHhitu6deuKz75//77Wx4lxo0GA0CBAaBAgNAgQGgQIDQL8eH8KqvYvp86cOVNx27p1a/HZRYsWjetM/8bBgweLe19fX92+u97caBAgNAgQGgQIDQKEBgFCgwChQcCUfY82Z86c4v7jx4/ivmDBguJe7d8b/ar/nmjt2rXFfXBwsG7f/eXLl+K+b9++4n7z5s1aHqem3GgQIDQIEBoECA0ChAYBQoMAoUHAH40+QL3s2LGjuLe3txf3zs7O4l7tT59dvny54nbu3Lnis58/fy7u9bRmzZqGfXdLS0txnzt3bugktedGgwChQYDQIEBoECA0CBAaBAgNAqbse7Rqqr0nq2bevHnFvbu7u+K2Z8+e4rPVfleunubPn9+w757K3GgQIDQIEBoECA0ChAYBQoMAoUHAlP27jq2trcV93bp1xX3Dhg3FvaOjY8xnoqynp6e43759u7i/fPmylsepKTcaBAgNAoQGAUKDAKFBgNAgYMr+eH+iZs2aVdxXrlxZ3E+fPl1xW79+ffHZGTNmFPdGevXqVXF/+PBhcT9y5EjFbWRkpPhsI399aKLcaBAgNAgQGgQIDQKEBgFCgwChQYD3aA2wZcuW4j5z5szivnr16uLe1dVV3AcGBipuV65cKT47NDRU3J8/f17cf1duNAgQGgQIDQKEBgFCgwChQYDQIMB7NAhwo0GA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQL+CyYg+k7RzLLnAAAAAElFTkSuQmCC\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m27a650940f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m27a650940f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m27a650940f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m27a650940f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m27a650940f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m27a650940f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m27a650940f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mefd7d9aded\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mefd7d9aded\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mefd7d9aded\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mefd7d9aded\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mefd7d9aded\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mefd7d9aded\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mefd7d9aded\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p63a874339f\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAORklEQVR4nO3df4xVdXrH8c/TKYsG9g9YLRlBZReJZtNQtiHGpNJIyK5WYoAYDUSNUpLxjzUupkkHt1FAWWNqrQn/EGeFMG2oK0ZWzcaEtYDFamJEYhWd7koRBTIwKiYzmOgqPP1jDmbAOd873HPOPRee9yuZzL3nmXPOkxs+nHPPr6+5uwCc//6s7gYAtAZhB4Ig7EAQhB0IgrADQfx5K1dmZhz6Byrm7jba9EJbdjO7wcz+YGb7zGxlkWUBqJY1e57dzDok/VHSTyUdkvSmpKXu/n5iHrbsQMWq2LJfLWmfu+939z9J+o2khQWWB6BCRcI+VdLBEe8PZdNOY2ZdZrbbzHYXWBeAgio/QOfuPZJ6JHbjgToV2bIflnTpiPfTsmkA2lCRsL8paaaZ/dDMvidpiaQXy2kLQNma3o1392/M7B5J2yR1SNro7u+V1hmAUjV96q2plfGdHahcJRfVADh3EHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQREuHbEbr3Xzzzcn6BRdckKzPmTMnWV+xYkWyvnPnztzahg0bkvP29fUl63v27EnWcTq27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBKO4toELL7wwWb/yyiuT9Ycffji3Nn/+/OS848ePT9br9OGHHybrO3bsSNa7u7tza4ODg8l5T5w4kay3s7xRXAtdVGNmByQNSToh6Rt3T1+BAaA2ZVxBN8/dPy1hOQAqxHd2IIiiYXdJvzezt8ysa7Q/MLMuM9ttZrsLrgtAAUV3469198Nm9heSXjaz/3X3XSP/wN17JPVIHKAD6lRoy+7uh7PfA5J+K+nqMpoCUL6mw25mE8zs+6deS/qZpL1lNQagXE2fZzezH2l4ay4Nfx34D3f/VYN5zsvd+FmzZiXrc+fOTdavv/76ZH3BggVn3RPS1qxZk6xv3bo1Wd+7t323a6WfZ3f3/ZL+qumOALQUp96AIAg7EARhB4Ig7EAQhB0IgkdJl6DRqbV169a1qJPv+vjjj5P1Om/l7OzsTNYbPea6iFWrViXrn3zySbLezqfe8rBlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgOM/eAs8//3yyvmjRomT9yJEjyfpTTz2VW3vssceS8x4/fjxZr9K9996brD/xxBMt6iQGtuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARDNpdg0qRJyXqje8YvueSSZP3LL79M1g8cOJCst6trrrkmWX/ttdcqW/cXX3yRrC9fvjxZf/bZZ8tsp1R5j5Jmyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQXA/ewk+//zzQvMPDg6W1EnrjRs3Lll/5JFHcmu33HJL2e2MWXd3d7LezufRm9Vwy25mG81swMz2jpg22cxeNrMPst/pq0oA1G4su/GbJN1wxrSVkra7+0xJ27P3ANpYw7C7+y5Jx86YvFBSb/a6V9KictsCULZmv7NPcff+7PURSVPy/tDMuiR1NbkeACUpfIDO3T11g4u790jqkc7fG2GAc0Gzp96OmlmnJGW/B8prCUAVmg37i5LuzF7fKemFctoBUJWG97Ob2dOSrpN0kaSjklZJel7SFkmXSfpI0q3ufuZBvNGWxW78OWbevHnJ+n333ZesL1iwoMx2zsr+/ftza3Pnzk3O2+hZ/e0s7372ht/Z3X1pTml+oY4AtBSXywJBEHYgCMIOBEHYgSAIOxAEt7gGt2zZsmT9ySefTNY7OjrKbOesPPTQQ8l6aqjsc/nUWrPYsgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEJxnPw/MmjUrt7Zw4cLkvA888ECyXuV59EZDUb/00kvJem9vb7J+rg5lXRW27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRMNHSZe6Mh4lPapGwx7PmDEjWX/hhfzH9l9xxRVN9XTKiRMnkvWvv/666WU/+OCDyfrjjz/e9LIjy3uUNFt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiC+9nbQHd3d7K+Zs2aytb96quvJuvPPPNMsr5+/foy20GFGm7ZzWyjmQ2Y2d4R01ab2WEzezv7ubHaNgEUNZbd+E2Sbhhl+hPuPjv7ST9SBEDtGobd3XdJOtaCXgBUqMgBunvM7J1sN39S3h+ZWZeZ7Taz3QXWBaCgZsO+XtIMSbMl9UvKvWPB3XvcfY67z2lyXQBK0FTY3f2ou59w95OSfi3p6nLbAlC2psJuZp0j3i6WtDfvbwG0h4bn2c3saUnXSbrIzA5JWiXpOjObLcklHZB0d3Uttr8JEyYk643uKb/rrrtK7OZ0O3fuTNbvuOOOZL2/v7/MdlCjhmF396WjTN5QQS8AKsTlskAQhB0IgrADQRB2IAjCDgTBLa4laHTqbN26dZWu/5VXXsmtLV68ODnv0NBQyd2gXbFlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGLJ5jK666qrc2rZt25LzTps2rdC6t2/fnqzffvvtubWBgYFC667S5Zdfnqw3unV47dq1hZZfxPHjx5P1+++/P1l//fXXy2znNAzZDARH2IEgCDsQBGEHgiDsQBCEHQiCsANBcD97Zvbs2cn6li1bcmtFz6M3sm/fvmR95syZubWi59lXr16drHd0dDS97Ntuuy1Zr/I8eVHLli1L1qs8j94stuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATn2TONzmXv2LEjtzZjxoyy2znN3XenR8S+9dZbc2uDg4OF1n3ZZZcl62aj3jp93ps6dWrdLZy1hlt2M7vUzHaa2ftm9p6Z/SKbPtnMXjazD7Lfk6pvF0CzxrIb/42kf3D3H0u6RtLPzezHklZK2u7uMyVtz94DaFMNw+7u/e6+J3s9JKlP0lRJCyX1Zn/WK2lRRT0CKMFZfWc3s+mSfiLpDUlT3L0/Kx2RNCVnni5JXQV6BFCCMR+NN7OJkp6TtMLdTzvq48NPrRz1YZLu3uPuc9x9TqFOARQyprCb2TgNB32zu2/NJh81s86s3impfR9jCqDxo6Rt+NxKr6Rj7r5ixPTHJH3m7o+a2UpJk939Hxss65x9lPT48eNza5s2bUrOmzo1hnqsWrUqWf/ss8+S9Y0bNybrX3311Vn3VJa8R0mP5Tv730i6Q9K7ZvZ2Nu2Xkh6VtMXMlkv6SBL/ooE21jDs7v7fkvKunJhfbjsAqsLlskAQhB0IgrADQRB2IAjCDgTBLa5jlDpvunnz5uS8F198cbI+b968pno6Fxw8eDC3tmTJkuS8fX19ZbfzraGhoWT95MmTla27LmzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIhvezl7qyc/h+9iImTpyYrN90003J+vTp05P1tWvXnm1L3+rp6UnWd+3a1fSyJWn//v25tTfeeKPQsjG6vPvZ2bIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCcZwfOM5xnB4Ij7EAQhB0IgrADQRB2IAjCDgRB2IEgGobdzC41s51m9r6ZvWdmv8imrzazw2b2dvZzY/XtAmhWw4tqzKxTUqe77zGz70t6S9IiDY/Hftzd/2XMK+OiGqByeRfVjGV89n5J/dnrITPrkzS13PYAVO2svrOb2XRJP5F06nlC95jZO2a20cwm5czTZWa7zWx3sVYBFDHma+PNbKKk/5L0K3ffamZTJH0qySU9rOFd/b9vsAx244GK5e3GjynsZjZO0u8kbXP3fx2lPl3S79z9Lxssh7ADFWv6RhgzM0kbJPWNDHp24O6UxZL2Fm0SQHXGcjT+WkmvSnpX0qlxbH8paamk2RrejT8g6e7sYF5qWWzZgYoV2o0vC2EHqsf97EBwhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAaPnCyZJ9K+mjE+4uyae2oXXtr174kemtWmb1dnldo6f3s31m52W53n1NbAwnt2lu79iXRW7Na1Ru78UAQhB0Iou6w99S8/pR27a1d+5LorVkt6a3W7+wAWqfuLTuAFiHsQBC1hN3MbjCzP5jZPjNbWUcPeczsgJm9mw1DXev4dNkYegNmtnfEtMlm9rKZfZD9HnWMvZp6a4thvBPDjNf62dU9/HnLv7ObWYekP0r6qaRDkt6UtNTd329pIznM7ICkOe5e+wUYZva3ko5L+rdTQ2uZ2T9LOubuj2b/UU5y9+426W21znIY74p6yxtm/C7V+NmVOfx5M+rYsl8taZ+773f3P0n6jaSFNfTR9tx9l6RjZ0xeKKk3e92r4X8sLZfTW1tw935335O9HpJ0apjxWj+7RF8tUUfYp0o6OOL9IbXXeO8u6fdm9paZddXdzCimjBhm64ikKXU2M4qGw3i30hnDjLfNZ9fM8OdFcYDuu65197+W9HeSfp7trrYlH/4O1k7nTtdLmqHhMQD7JT1eZzPZMOPPSVrh7oMja3V+dqP01ZLPrY6wH5Z06Yj307JpbcHdD2e/ByT9VsNfO9rJ0VMj6Ga/B2ru51vuftTdT7j7SUm/Vo2fXTbM+HOSNrv71mxy7Z/daH216nOrI+xvSpppZj80s+9JWiLpxRr6+A4zm5AdOJGZTZD0M7XfUNQvSroze32npBdq7OU07TKMd94w46r5s6t9+HN3b/mPpBs1fET+/yT9Ux095PT1I0n/k/28V3dvkp7W8G7d1xo+trFc0g8kbZf0gaT/lDS5jXr7dw0P7f2OhoPVWVNv12p4F/0dSW9nPzfW/dkl+mrJ58blskAQHKADgiDsQBCEHQiCsANBEHYgCMIOBEHYgSD+H9SJgeRhz6mZAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get a random example image\n",
    "plt.imshow(x[0].numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training & validation sets using PyTorch's `random_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's `DataLoader` \n",
    "PyTorch has a handy utility called a `DataLoader` which can pass us our data in mini-batches of a specified batch size. It can also shuffle them for us.\n",
    "\n",
    "Let's use `torch.data.DataLoader` to create data loaders from our train, validation and test datasets now. Hint: look at the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader( # create a data loader\n",
    "    train_data, # what dataset should it sample from?\n",
    "    shuffle=True, # should it shuffle the examples?\n",
    "    batch_size=batch_size # how large should the batches that it samples be?\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a neural network with PyTorch\n",
    "\n",
    "PyTorch makes it really easy for us to build complex models that can be improved via gradient based optimisation. It does this by providing a class named `torch.nn.Module`. Our model classes should inherit from this class because it does a few very useful things for us:\n",
    "\n",
    "1. `torch.nn.Module` keeps track of all `torch.nn.Parameters` that are created within it. So when we add a linear layer to our model, the parameters (matrix of weights) in that layer will be added to a list of our model's parameters. We can retrieve all parameters of our model using its `parameters()` method. We will later pass this (`mymodel.parameters()`) to our optimiser when we tell it that *this* is what it should be optimising.\n",
    "\n",
    "\n",
    "2. `torch.nn.Module` treats the `forward` method (function) of any child class specially by assigning it to the `__call__` method. That means that running `mymodel.forward(some_data)` is equal to `mymodel(some_data)`. \n",
    "\n",
    "\n",
    "It contains many more useful tools\n",
    "\n",
    "[More detail](https://pytorch.org/tutorials/beginner/nn_tutorial.html) on `torch.nn.Module`\n",
    "Check out the docs [here]()\n",
    "\n",
    "Once we have created a class to represent our model, we need to define how it performs the forward pass. What layers of transformations do we need to give it? \n",
    "Check out these [docs](https://pytorch.org/docs/stable/nn.html#linear-layers) to look at all the layers PyTorch provides.\n",
    "Hint: what layer have I linked to?\n",
    "\n",
    "After we've defined some layers for our model we should implement the forward function that will define what happens when we call an instance of our class. This should pass the argument (our input data) through each of the layers, and apply an activation function to them between each, before returning the transformed input as the output. The output should represent a categorical probability distribution over which class the input belongs to. What shape does it need to be? What function does it need to have applied to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "class NN(torch.nn.Module): # create a neural network class\n",
    "    def __init__(self): # initialiser\n",
    "        super().__init__() # initialise the parent class\n",
    "        self.layer1 = torch.nn.Linear(784, 1024) # create our first linear layer\n",
    "        self.layer2 = torch.nn.Linear(1024, 256) # create our second linear layer\n",
    "        self.layer3 = torch.nn.Linear(256, 10) # create our third linear layer\n",
    "        \n",
    "    def forward(self, x): # define the forward pass\n",
    "        x = x.view(-1, 784) # flatten out our image features into vectors\n",
    "        x = self.layer1(x) # pass through the first linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer2(x) # pass through the second linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer3(x) # pass through the third linear layer\n",
    "        x = F.softmax(x) # apply activation function\n",
    "        return x # return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network and visualising it's performance\n",
    "\n",
    "Now we've actually made a template for our model, we can actually\n",
    "- instantiate it by creating an instance of it from our class template\n",
    "- define how we will improve it by specifying an optimiser\n",
    "- define how we will measure its performance by specifying a criterion\n",
    "- train it\n",
    "- write its loss to a graph and see how this changes as it continues to train\n",
    "\n",
    "Let's code that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'NN' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f64a824f5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# CREATE OUR OPTIMISER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NN' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "my_nn = NN()\n",
    "\n",
    "# CREATE OUR OPTIMISER\n",
    "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
    "    myNeuralNetwork.parameters(),          # what should it optimise?\n",
    "    lr=learning_rate                       # using what learning rate?\n",
    ")\n",
    "\n",
    "# CREATE OUR CRITERION\n",
    "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='../runs')                            # we will use this to show our models performance on a graph\n",
    "    \n",
    "# TRAINING LOOP\n",
    "def train(model, epochs):\n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "train(my_nn, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the loss actually mean practically?\n",
    "\n",
    "The absolute value of the loss doesn't really mean much, it's just a way of continuously evaluating the relative performance of the model whilst it trains. The real metric of performance that we care about is the proportion of ***unseen*** examples that our neural network can correctly classify. These unseen examples are what the test loader consists of.\n",
    "\n",
    "Let's write the code to calculate that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def test(model):\n",
    "    num_correct = 0\n",
    "    num_examples = len(test_data)                       # test DATA not test LOADER\n",
    "    for inputs, labels in test_loader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    print('Accuracy:', percent_correct)\n",
    "    \n",
    "test(myNeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Compare the loss curves generated by using different batch sizes. What's the best? As you change the batch size, what variable do you need to change to give those curves the same domain over the x-axis (num writes to summary writer)\n",
    "2. It would be good to validate our model as we go along to ensure that we don't overfit. Let's write a training loop that tests the loss on the validation set after each epoch. Plot the validation error alongside What can you see on the graphs that indicates overfitting?\n",
    "3. What is the best accuracy you can achieve? Can you implement a grid search and a random search to try them automatically. Record all permutations that you try.\n",
    "4. What feature of the input **image** data is our standard neural network not taking advantage of? Hint: '************* neural networks' take this into account.\n",
    "5. Implement another neural network to predict the miles per gallon achieved by a car from [this](https://archive.ics.uci.edu/ml/datasets/Auto+MPG) dataset. This is a regression problem - how does this change the implementation?. \n",
    "\n",
    "## Congratulations you boss, you've finished the notebook!\n",
    "\n",
    "Please provide your feedback [here](https://docs.google.com/forms/d/e/1FAIpQLSdZSxvkAE19vjDN4jpp0VvUBPGr_wdtayGAcRNfFGH7e7jQDQ/viewform?usp=sf_link). It means a lot to us.\n",
    "\n",
    "Next, you might want to check out:\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Convolutional%20Neural%20Networks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}