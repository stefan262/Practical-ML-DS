# Optimisation for Deep Learning

## Recap on optimisation
- ask what is optimisation?
- the difference between learning and pure optimisation
- remind everyone of grid search and highlight how it is awful for models with large numbers of parameters
    - the need for minibatches
- recap gradient descent and SGD

## Challenges with NN optimisation
- Ill conditioning
    - loss surface changes much faster in some directions than others
- Local minima
    - 

## SGD theory

## Momentum

## AdaGrad

## RMSProp

## Adam

## Summary
- different optimisers work better in different scenarios
- in general use Adam or SGD with momentum

## Challenge: plot the loss curves for different optimisers when training a neural network

## Implementing our own optimiser