{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit2b4c68a66e834f80ac62a395bc4bcb45",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimisation for Deep Learning\n",
    "\n",
    "Learning outcomes\n",
    "- understand mathematically and intuitively the most common optimisation algorithms used for optimising deep models\n",
    "- implement your own optimiser in PyTorch\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SGD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## AdaGrad\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RMSProp\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Adam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## So which algorithm do I use?\n",
    "\n",
    "Well... as usual, it depends on your problem and your dataset.\n",
    "\n",
    "It's still a highly active field of research. But in general, **SGD with momentum or Adam** are the go to choices for optimising deep models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using these optimisation algorithms\n",
    "\n",
    "Let's set up the same neural network as in the previous module, and then switch out the optimiser for Adam and others and show how you can adapt it to use momentum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import NN, get_dataloaders\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# HOW TO USE DIFFERENT OPTIMISERS PROVIDED BY PYTORCH\n",
    "optimiser = torch.optim.SGD(my_nn.parameters(), lr=learning_rate, momentum=0.1)\n",
    "# optimiser = torch.optim.Adagrad(NN.parameters(), lr=learning_rate)\n",
    "# optimiser = torch.optim.RMSprop(NN.parameters(), lr=learning_rate)\n",
    "optimiser = torch.optim.Adam(my_nn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "The stuff below is exactly the same as before!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GET DATALOADERS\n",
    "test_loader, val_loader, train_loader = get_dataloaders()\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, optimiser, graph_name, epochs=1, tag='Loss/Train'):\n",
    "    writer = SummaryWriter(log_dir=f'../../runs/{tag}') # make a different writer for each tagged optimisation run\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar(f'Loss/{graph_name}', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "# train(my_nn, optimiser)"
   ]
  },
  {
   "source": [
    "Let's compare the training curves generated using some of the optimisers that we explained above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " \tLoss: tensor(2.1468, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 340 \tLoss: tensor(2.1868, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 341 \tLoss: tensor(2.2765, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 342 \tLoss: tensor(2.1437, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 343 \tLoss: tensor(2.2342, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 344 \tLoss: tensor(2.1550, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 345 \tLoss: tensor(2.1320, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 346 \tLoss: tensor(2.1429, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 347 \tLoss: tensor(2.0624, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 348 \tLoss: tensor(2.1047, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 349 \tLoss: tensor(2.0750, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 350 \tLoss: tensor(2.1382, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 351 \tLoss: tensor(2.1334, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 352 \tLoss: tensor(2.1329, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 353 \tLoss: tensor(2.2057, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 354 \tLoss: tensor(2.2102, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 355 \tLoss: tensor(2.1677, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 356 \tLoss: tensor(2.1961, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 357 \tLoss: tensor(2.1522, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 358 \tLoss: tensor(2.1036, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 359 \tLoss: tensor(2.1374, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 360 \tLoss: tensor(2.2043, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 361 \tLoss: tensor(2.0458, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 362 \tLoss: tensor(2.1801, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 363 \tLoss: tensor(2.1346, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 364 \tLoss: tensor(2.1695, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 365 \tLoss: tensor(2.1196, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 366 \tLoss: tensor(2.0542, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 367 \tLoss: tensor(2.0839, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 368 \tLoss: tensor(2.1975, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 369 \tLoss: tensor(2.1306, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 370 \tLoss: tensor(2.1295, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 371 \tLoss: tensor(2.0087, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 372 \tLoss: tensor(2.1586, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 373 \tLoss: tensor(2.0870, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 374 \tLoss: tensor(2.1670, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 375 \tLoss: tensor(2.0402, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 376 \tLoss: tensor(2.1127, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 377 \tLoss: tensor(2.1742, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 378 \tLoss: tensor(2.1445, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 379 \tLoss: tensor(2.1241, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 380 \tLoss: tensor(2.1318, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 381 \tLoss: tensor(2.1343, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 382 \tLoss: tensor(2.0446, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 383 \tLoss: tensor(2.1274, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 384 \tLoss: tensor(2.1257, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 385 \tLoss: tensor(2.0620, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 386 \tLoss: tensor(2.1675, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 387 \tLoss: tensor(2.1585, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 388 \tLoss: tensor(2.0687, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 389 \tLoss: tensor(2.0503, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 390 \tLoss: tensor(2.1062, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 391 \tLoss: tensor(2.2002, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 392 \tLoss: tensor(2.1733, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 393 \tLoss: tensor(2.0487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 394 \tLoss: tensor(2.0249, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 395 \tLoss: tensor(2.1493, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 396 \tLoss: tensor(2.0214, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 397 \tLoss: tensor(2.0767, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 398 \tLoss: tensor(2.0291, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 399 \tLoss: tensor(2.0276, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 400 \tLoss: tensor(2.0548, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 401 \tLoss: tensor(2.1229, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 402 \tLoss: tensor(2.1268, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 403 \tLoss: tensor(2.1063, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 404 \tLoss: tensor(2.0974, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 405 \tLoss: tensor(2.0676, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 406 \tLoss: tensor(2.1042, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 407 \tLoss: tensor(2.0291, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 408 \tLoss: tensor(1.9722, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 409 \tLoss: tensor(2.1190, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 410 \tLoss: tensor(2.1507, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 411 \tLoss: tensor(2.0493, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 412 \tLoss: tensor(2.1452, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 413 \tLoss: tensor(2.1304, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 414 \tLoss: tensor(2.1410, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 415 \tLoss: tensor(2.1204, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 416 \tLoss: tensor(2.1019, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 417 \tLoss: tensor(2.2174, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 418 \tLoss: tensor(2.0870, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 419 \tLoss: tensor(1.9803, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 420 \tLoss: tensor(2.0456, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 421 \tLoss: tensor(2.0986, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 422 \tLoss: tensor(2.0672, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 423 \tLoss: tensor(2.0790, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 424 \tLoss: tensor(2.0350, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 425 \tLoss: tensor(2.1392, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 426 \tLoss: tensor(2.0219, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 427 \tLoss: tensor(2.0589, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 428 \tLoss: tensor(2.0904, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 429 \tLoss: tensor(2.1018, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 430 \tLoss: tensor(2.0844, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 431 \tLoss: tensor(1.9902, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 432 \tLoss: tensor(2.0157, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 433 \tLoss: tensor(2.1096, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 434 \tLoss: tensor(1.9989, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 435 \tLoss: tensor(2.0688, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 436 \tLoss: tensor(2.0541, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 437 \tLoss: tensor(2.1215, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 438 \tLoss: tensor(2.0094, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 439 \tLoss: tensor(2.0220, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 440 \tLoss: tensor(2.1198, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 441 \tLoss: tensor(2.0352, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 442 \tLoss: tensor(2.0433, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 443 \tLoss: tensor(2.0387, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 444 \tLoss: tensor(2.0287, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 445 \tLoss: tensor(2.1281, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 446 \tLoss: tensor(2.0730, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 447 \tLoss: tensor(2.0055, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 448 \tLoss: tensor(2.0881, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 449 \tLoss: tensor(2.0050, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 450 \tLoss: tensor(1.9893, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 451 \tLoss: tensor(2.1214, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 452 \tLoss: tensor(2.0676, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 453 \tLoss: tensor(1.9589, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 454 \tLoss: tensor(2.0412, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 455 \tLoss: tensor(2.0722, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 456 \tLoss: tensor(2.0682, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 457 \tLoss: tensor(2.1743, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 458 \tLoss: tensor(1.9792, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 459 \tLoss: tensor(1.9792, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 460 \tLoss: tensor(2.0026, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 461 \tLoss: tensor(2.0194, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 462 \tLoss: tensor(2.1015, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 463 \tLoss: tensor(1.9709, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 464 \tLoss: tensor(1.9865, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 465 \tLoss: tensor(2.0324, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 466 \tLoss: tensor(2.1269, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 467 \tLoss: tensor(2.1038, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 468 \tLoss: tensor(2.0510, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 469 \tLoss: tensor(1.8806, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 470 \tLoss: tensor(1.9887, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 471 \tLoss: tensor(1.9608, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 472 \tLoss: tensor(2.1010, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 473 \tLoss: tensor(1.9257, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 474 \tLoss: tensor(1.9271, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 475 \tLoss: tensor(2.1310, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 476 \tLoss: tensor(2.0843, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 477 \tLoss: tensor(2.0310, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 478 \tLoss: tensor(2.0312, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 479 \tLoss: tensor(2.0521, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 480 \tLoss: tensor(2.0277, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 481 \tLoss: tensor(2.0380, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 482 \tLoss: tensor(2.1532, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 483 \tLoss: tensor(2.0890, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 484 \tLoss: tensor(1.9795, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 485 \tLoss: tensor(2.1200, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 486 \tLoss: tensor(2.0892, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 487 \tLoss: tensor(1.9444, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 488 \tLoss: tensor(2.1351, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 489 \tLoss: tensor(1.9349, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 490 \tLoss: tensor(2.0759, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 491 \tLoss: tensor(1.9841, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 492 \tLoss: tensor(2.1134, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 493 \tLoss: tensor(1.9190, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 494 \tLoss: tensor(1.9432, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 495 \tLoss: tensor(1.9714, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 496 \tLoss: tensor(1.9769, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 497 \tLoss: tensor(2.0034, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 498 \tLoss: tensor(1.9946, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 499 \tLoss: tensor(1.8834, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 500 \tLoss: tensor(2.0098, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 501 \tLoss: tensor(1.9822, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 502 \tLoss: tensor(2.0142, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 503 \tLoss: tensor(1.9993, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 504 \tLoss: tensor(2.0079, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 505 \tLoss: tensor(2.0181, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 506 \tLoss: tensor(1.9313, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 507 \tLoss: tensor(1.9879, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 508 \tLoss: tensor(2.0017, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 509 \tLoss: tensor(2.1316, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 510 \tLoss: tensor(2.0139, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 511 \tLoss: tensor(1.9531, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 512 \tLoss: tensor(1.9586, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 513 \tLoss: tensor(2.0431, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 514 \tLoss: tensor(1.8839, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 515 \tLoss: tensor(2.1204, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 516 \tLoss: tensor(2.0038, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 517 \tLoss: tensor(1.9738, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 518 \tLoss: tensor(1.9463, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 519 \tLoss: tensor(1.9937, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 520 \tLoss: tensor(1.9517, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 521 \tLoss: tensor(1.9817, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 522 \tLoss: tensor(2.1573, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 523 \tLoss: tensor(2.0908, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 524 \tLoss: tensor(2.1238, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 525 \tLoss: tensor(2.0191, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 526 \tLoss: tensor(2.0961, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 527 \tLoss: tensor(1.9641, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 528 \tLoss: tensor(1.9459, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 529 \tLoss: tensor(1.9456, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 530 \tLoss: tensor(1.8715, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 531 \tLoss: tensor(2.0361, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 532 \tLoss: tensor(2.0054, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 533 \tLoss: tensor(2.0413, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 534 \tLoss: tensor(2.0688, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 535 \tLoss: tensor(1.9915, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 536 \tLoss: tensor(2.0313, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 537 \tLoss: tensor(1.8103, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 538 \tLoss: tensor(1.9616, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 539 \tLoss: tensor(1.8593, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 540 \tLoss: tensor(1.9567, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 541 \tLoss: tensor(1.9671, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 542 \tLoss: tensor(1.9928, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 543 \tLoss: tensor(2.1066, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 544 \tLoss: tensor(2.0848, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 545 \tLoss: tensor(1.9076, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 546 \tLoss: tensor(1.9512, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 547 \tLoss: tensor(1.9819, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 548 \tLoss: tensor(1.9902, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 549 \tLoss: tensor(2.1027, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 550 \tLoss: tensor(1.9882, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 551 \tLoss: tensor(2.0053, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 552 \tLoss: tensor(2.0879, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 553 \tLoss: tensor(2.0182, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 554 \tLoss: tensor(2.0041, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 555 \tLoss: tensor(1.9290, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 556 \tLoss: tensor(2.0547, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 557 \tLoss: tensor(2.1109, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 558 \tLoss: tensor(2.0088, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 559 \tLoss: tensor(1.9651, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 560 \tLoss: tensor(1.9302, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 561 \tLoss: tensor(1.9235, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 562 \tLoss: tensor(2.1260, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 563 \tLoss: tensor(1.9904, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 564 \tLoss: tensor(2.0537, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 565 \tLoss: tensor(1.9858, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 566 \tLoss: tensor(1.9420, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 567 \tLoss: tensor(1.9643, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 568 \tLoss: tensor(1.9188, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 569 \tLoss: tensor(1.9479, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 570 \tLoss: tensor(1.9780, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 571 \tLoss: tensor(1.9571, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 572 \tLoss: tensor(2.0327, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 573 \tLoss: tensor(1.9533, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 574 \tLoss: tensor(1.9668, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 575 \tLoss: tensor(1.9958, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 576 \tLoss: tensor(1.9415, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 577 \tLoss: tensor(1.9878, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 578 \tLoss: tensor(1.9859, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 579 \tLoss: tensor(1.9673, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 580 \tLoss: tensor(1.9863, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 581 \tLoss: tensor(2.0067, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 582 \tLoss: tensor(1.9019, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 583 \tLoss: tensor(1.9790, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 584 \tLoss: tensor(1.9025, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 585 \tLoss: tensor(1.8801, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 586 \tLoss: tensor(1.8478, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 587 \tLoss: tensor(1.8573, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 588 \tLoss: tensor(1.8650, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 589 \tLoss: tensor(1.9614, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 590 \tLoss: tensor(1.9132, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 591 \tLoss: tensor(2.0537, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 592 \tLoss: tensor(1.9176, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 593 \tLoss: tensor(1.8860, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 594 \tLoss: tensor(2.0021, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 595 \tLoss: tensor(1.9406, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 596 \tLoss: tensor(1.9487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 597 \tLoss: tensor(1.9847, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 598 \tLoss: tensor(2.0373, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 599 \tLoss: tensor(1.8970, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 600 \tLoss: tensor(1.9421, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 601 \tLoss: tensor(2.0246, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 602 \tLoss: tensor(2.0737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 603 \tLoss: tensor(1.9819, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 604 \tLoss: tensor(1.9056, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 605 \tLoss: tensor(1.9112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 606 \tLoss: tensor(2.0045, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 607 \tLoss: tensor(1.9631, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 608 \tLoss: tensor(1.9168, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 609 \tLoss: tensor(2.0182, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 610 \tLoss: tensor(1.8586, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 611 \tLoss: tensor(1.8446, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 612 \tLoss: tensor(2.1088, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 613 \tLoss: tensor(1.9870, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 614 \tLoss: tensor(1.8607, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 615 \tLoss: tensor(1.8028, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 616 \tLoss: tensor(1.9675, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 617 \tLoss: tensor(1.8747, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 618 \tLoss: tensor(2.0497, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 619 \tLoss: tensor(1.9678, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 620 \tLoss: tensor(1.7743, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 621 \tLoss: tensor(1.8079, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 622 \tLoss: tensor(1.8807, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 623 \tLoss: tensor(1.9691, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 624 \tLoss: tensor(1.8102, grad_fn=<NllLossBackward>)\n"
    }
   ],
   "source": [
    "optimisers = [\n",
    "    {\n",
    "        'optimiser_class': torch.optim.SGD, \n",
    "        'tag': 'SGD'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adam,\n",
    "        'tag': 'Adam'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adagrad,\n",
    "        'tag': 'Adagrad'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.RMSprop,\n",
    "        'tag': 'RMSProp'\n",
    "    }\n",
    "]\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for optimiser_obj in optimisers:   \n",
    "    for lr in learning_rates:\n",
    "        my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "        optimiser_class = optimiser_obj['optimiser_class']\n",
    "        optimiser = optimiser_class(my_nn.parameters(), lr=lr)\n",
    "        tag = optimiser_obj['tag']\n",
    "        train(my_nn, optimiser, graph_name=lr, epochs=1, tag=f'Loss/Train/{tag}')\n",
    "    "
   ]
  },
  {
   "source": [
    "## Implementing our own PyTorch optimiser\n",
    "\n",
    "To understand a bit further what's happening under the hood, let's implement SGD from scratch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, model_params, learning_rate):\n",
    "        self.model_params = model_params\n",
    "        self.learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.model_params:\n",
    "            param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model_params:\n",
    "            param.grad = torch.zeros_like(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = NN()\n",
    "optimiser = SGD()\n",
    "\n",
    "train(my_nn, optimiser)"
   ]
  }
 ]
}