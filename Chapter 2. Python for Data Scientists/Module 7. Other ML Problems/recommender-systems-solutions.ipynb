{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Introduction: Simple Recommender Systems](#Introduction:-Simple-Recommeder-Systems)\n",
    "- [Offline & Online Evaluation](#Offline-&-Online-Evaluation)\n",
    "- [Content-based Recommenders](#Content\\-based-Recommenders)\n",
    "- [Collaborative-filtering](#Collaborative\\-filtering)\n",
    "- [Hybrid Systems](#Hybrid-Systems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Simple Recommender Systems\n",
    "\n",
    "__Recommender systems__, also referred to as __recommendation systems__, are filtering systems used by many different companies world-wide to be able to recommend products (e.g. movies, clothes, etc) based on user preferences. Unlike __ranking algorithms,__ recommender systems aim to provide recommendations without an explicit input from the user (such as a search query). We obviously cannot recommend _exactly_ what a user wants as we cannot access or process all the information in their brain at the same time. Instead, we can take advantage or users' past ratings, choices and preferences to __predict__ the products the user will most probably like.\n",
    "\n",
    "How do these systems do what they do? This is question that has become a large topic of research and the current answer is that there are mutliple ways to create recommender systems: each working under different assumptions and algorithms. There are two main broad classifications that we will cover shortly: __content-based recommendation__ (item-centred) and __collaborative filtering__ (user-centred).\n",
    "\n",
    "<img width=\"500px;\" height=\"500px\" src=\"https://www.researchgate.net/profile/Lionel_Ngoupeyou_Tondji/publication/323726564/figure/fig5/AS:631605009846299@1527597777415/Content-based-filtering-vs-Collaborative-filtering-Source.png\">\n",
    "\n",
    "[Source](https://www.researchgate.net/publication/323726564_Web_Recommender_System_for_Job_Seeking_and_Recruiting)\n",
    "\n",
    "Before we cover the implementation of these, we will cover what is informally referred to as a simple recommender system: a system that uses the weighted average rating from all users to make recommendations on the \"best\" options. This is also referred to as a type of __demographic recommender system.__ Throughout this notebook, we will use the \"Movies Dataset\" from [Kaggle](https://www.kaggle.com/rounakbanik/the-movies-dataset), where the full version contains information on over 45,000 movies with 26 million ratings from  270,000 users. We will be using the small version, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179\n",
       "2       1     1061     3.0  1260759182\n",
       "3       1     1129     2.0  1260759185\n",
       "4       1     1172     4.0  1260759205"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing ratings\n",
    "ratings = pd.read_csv(\"../DATA/ratings_small.csv\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15602</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>en</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>False</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31357</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>en</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11862</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>en</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>76578911.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
       "3  False                                                NaN  16000000   \n",
       "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                               homepage     id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
       "1                                   NaN   8844  tt0113497                en   \n",
       "2                                   NaN  15602  tt0113228                en   \n",
       "3                                   NaN  31357  tt0114885                en   \n",
       "4                                   NaN  11862  tt0113041                en   \n",
       "\n",
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  ... release_date  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
       "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
       "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
       "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
       "\n",
       "       revenue runtime                                   spoken_languages  \\\n",
       "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
       "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "\n",
       "     status                                            tagline  \\\n",
       "0  Released                                                NaN   \n",
       "1  Released          Roll the dice and unleash the excitement!   \n",
       "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Released  Friends are the people who let you be yourself...   \n",
       "4  Released  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                         title  video vote_average vote_count  \n",
       "0                    Toy Story  False          7.7     5415.0  \n",
       "1                      Jumanji  False          6.9     2413.0  \n",
       "2             Grumpier Old Men  False          6.5       92.0  \n",
       "3            Waiting to Exhale  False          6.1       34.0  \n",
       "4  Father of the Bride Part II  False          5.7      173.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing movie metadata\n",
    "metadata = pd.read_csv(\"../DATA/movies_metadata.csv\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple recommender system, we will use the IMDB's known __weighted average formula__ used for their Top Movies Chart, given as follows:\n",
    "\n",
    "$$ R_{W} = (\\frac{v}{v + m})R + (\\frac{m}{v + m})C  $$\n",
    "\n",
    "Where:\n",
    "- $R_{W}$ is the weighted average movie rating\n",
    "- $v$ is the number of votes for that movie title\n",
    "- $m$ is the minimum number of votes required to be in the top Chart\n",
    "- $R$ is the average rating of that movie title\n",
    "- $C$ is the mean vote rating across all movies\n",
    "\n",
    "We can now begin our calculations to construct our simple recommender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.244896612406511"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing mean vote count across all movies\n",
    "vote_counts = metadata[metadata['vote_count'].notnull()]['vote_count'].astype('int')\n",
    "vote_averages = metadata[metadata['vote_average'].notnull()]['vote_average'].astype('int')\n",
    "C = vote_averages.mean()\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now choose a value for the minimum number. In this case, we will choose a value $m$ that gives us movies that have received more votes than 95% of the other remaining movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434.0\n"
     ]
    }
   ],
   "source": [
    "# Computing minimum number of votes required\n",
    "m = vote_counts.quantile(0.95)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the movies that are considered to be canditates for the top charts in our recommender system given our computed 'm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2274, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting all movies that have a votecount that is greater than our m value\n",
    "qualified = metadata[(metadata['vote_count'] >= m) & (metadata['vote_count'].notnull()) & (metadata['vote_average'].notnull())] \\\n",
    "                 [['title', 'release_date', 'vote_count', 'vote_average', 'popularity', 'genres']]\n",
    "qualified['vote_count'] = qualified['vote_count'].astype('int')\n",
    "qualified['vote_average'] = qualified['vote_average'].astype('int')\n",
    "qualified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>popularity</th>\n",
       "      <th>genres</th>\n",
       "      <th>weighted_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15480</th>\n",
       "      <td>Inception</td>\n",
       "      <td>2010-07-14</td>\n",
       "      <td>14075</td>\n",
       "      <td>8</td>\n",
       "      <td>29.1081</td>\n",
       "      <td>[{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...</td>\n",
       "      <td>7.917588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008-07-16</td>\n",
       "      <td>12269</td>\n",
       "      <td>8</td>\n",
       "      <td>123.167</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 28, 'name...</td>\n",
       "      <td>7.905871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22879</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>2014-11-05</td>\n",
       "      <td>11187</td>\n",
       "      <td>8</td>\n",
       "      <td>32.2135</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 18, '...</td>\n",
       "      <td>7.897107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>1999-10-15</td>\n",
       "      <td>9678</td>\n",
       "      <td>8</td>\n",
       "      <td>63.8696</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}]</td>\n",
       "      <td>7.881753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4863</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>2001-12-18</td>\n",
       "      <td>8892</td>\n",
       "      <td>8</td>\n",
       "      <td>32.0707</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>7.871787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>Indiana Jones and the Temple of Doom</td>\n",
       "      <td>1984-05-23</td>\n",
       "      <td>2841</td>\n",
       "      <td>7</td>\n",
       "      <td>15.8023</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 28, '...</td>\n",
       "      <td>6.767415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16129</th>\n",
       "      <td>The King's Speech</td>\n",
       "      <td>2010-09-06</td>\n",
       "      <td>2817</td>\n",
       "      <td>7</td>\n",
       "      <td>11.2604</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 36, 'name...</td>\n",
       "      <td>6.765698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Sunset Boulevard</td>\n",
       "      <td>1950-08-10</td>\n",
       "      <td>533</td>\n",
       "      <td>8</td>\n",
       "      <td>11.7098</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}]</td>\n",
       "      <td>6.763480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>Sin City</td>\n",
       "      <td>2005-04-01</td>\n",
       "      <td>2755</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0105</td>\n",
       "      <td>[{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...</td>\n",
       "      <td>6.761143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16833</th>\n",
       "      <td>Source Code</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>2752</td>\n",
       "      <td>7</td>\n",
       "      <td>9.79696</td>\n",
       "      <td>[{'id': 53, 'name': 'Thriller'}, {'id': 878, '...</td>\n",
       "      <td>6.760918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title release_date  \\\n",
       "15480                                          Inception   2010-07-14   \n",
       "12481                                    The Dark Knight   2008-07-16   \n",
       "22879                                       Interstellar   2014-11-05   \n",
       "2843                                          Fight Club   1999-10-15   \n",
       "4863   The Lord of the Rings: The Fellowship of the Ring   2001-12-18   \n",
       "...                                                  ...          ...   \n",
       "2006                Indiana Jones and the Temple of Doom   1984-05-23   \n",
       "16129                                  The King's Speech   2010-09-06   \n",
       "895                                     Sunset Boulevard   1950-08-10   \n",
       "9888                                            Sin City   2005-04-01   \n",
       "16833                                        Source Code   2011-03-30   \n",
       "\n",
       "       vote_count  vote_average popularity  \\\n",
       "15480       14075             8    29.1081   \n",
       "12481       12269             8    123.167   \n",
       "22879       11187             8    32.2135   \n",
       "2843         9678             8    63.8696   \n",
       "4863         8892             8    32.0707   \n",
       "...           ...           ...        ...   \n",
       "2006         2841             7    15.8023   \n",
       "16129        2817             7    11.2604   \n",
       "895           533             8    11.7098   \n",
       "9888         2755             7    15.0105   \n",
       "16833        2752             7    9.79696   \n",
       "\n",
       "                                                  genres  weighted_average  \n",
       "15480  [{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...          7.917588  \n",
       "12481  [{'id': 18, 'name': 'Drama'}, {'id': 28, 'name...          7.905871  \n",
       "22879  [{'id': 12, 'name': 'Adventure'}, {'id': 18, '...          7.897107  \n",
       "2843                       [{'id': 18, 'name': 'Drama'}]          7.881753  \n",
       "4863   [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...          7.871787  \n",
       "...                                                  ...               ...  \n",
       "2006   [{'id': 12, 'name': 'Adventure'}, {'id': 28, '...          6.767415  \n",
       "16129  [{'id': 18, 'name': 'Drama'}, {'id': 36, 'name...          6.765698  \n",
       "895                        [{'id': 18, 'name': 'Drama'}]          6.763480  \n",
       "9888   [{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...          6.761143  \n",
       "16833  [{'id': 53, 'name': 'Thriller'}, {'id': 878, '...          6.760918  \n",
       "\n",
       "[250 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing weighted average and determining top 250 chart\n",
    "def weighted_rating(x):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "qualified['weighted_average'] = qualified.apply(weighted_rating, axis=1)\n",
    "qualified = qualified.sort_values('weighted_average', ascending=False).head(250)\n",
    "qualified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now we have the top charts. This chart can be carried further to become a simple recommender system by recommending the top movies in the charts to all users. Is this a good recommender system? Not particularly. By taking a global weighted average we are able to determine which ones are considered the best on average, but we are unable to account for the individual preferences of the users. For instance, if I was a fan of exclusively Romcoms, I would be recommended only movies I dislike from the list above. The same would happen to people who really don't like action and thrillers. Therefore, we must use our data to instead be able to account for individual preferences! Therefore, simple recommender systems like this that generate global recommendations are generally only used for users who the system has collected little data from.\n",
    "\n",
    "By accounting for individual user preferences, we would likely achieve a higher score. But how can we determine which system is better? This leads us to the two methods of recommender system evaluation: __offline evaluation__ and __online evaluation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline & Online Evaluation\n",
    "\n",
    "How can we tell that our recommender system is doing what it is supposed to? There are two different approaches to evaluating our system:\n",
    "\n",
    "- __Offline evaluation:__ Use data we already have and evaluation metrics to compute numeric efectiveness measures that can be tuned for and/or compared. These are the same evaluation metrics which we have encountered and used to assess the performance of our models\n",
    "- __Online evaluation:__  involves using a live system, and tracking user-related behaviors such as dwell-times, click-through rates, and purchase conversions\n",
    "\n",
    "When carrying offline evaluation, we can split our data into a training and a test dataset just as we have seen before to ensure that we are tuning our systems appropriately. On the other hand, online evaluation enables us to capture aspects of the performance of our system that offline methods cannot. Whether offline evaluation, online evaluation or a combination of both is the best method to evaluate our system's performance still remains a topic of research. For the purposes of this notebook, we will only be covering simplistic forms of offline evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Recommenders\n",
    "We can now begin to understand the first sub-class of recommendation systems: __content-based recommenders.__ Let us look at the recommendation problem in the context of our movies dataset. It is intuitive to say that we would like to recommend romance movies to someone that has rated other romantic movies highly as opposed to action, or to recommend older films to users to who are fans of old classics, or even Batman movies to a Batman fan. In this context, we are looking at the characteristics (content) of each movie, and recommending movies that are similar to the previously highly rated movies by the same user. \n",
    "\n",
    "There are multiple approaches for the machinery of content-based recommenders. Most will either use the features of movies to predict whether you like or dislike a movie (classification) or to predict the rating the user would give to a movie they have not yet seen (__model-based__). Some might even use the features of a movie you have just watched and recommend the most similar movies to that given movie given their respective features (__memory-based__). We will be creating our own algorithm to predict the ratings of unseen movies and recommend those that are rated the highest.\n",
    "\n",
    "In modelling terms, we can frame the problem of recommendation as using the __features__ of movies watched by a user and the ratings given to each movie to __predict__ the rating the user would give to a movie not yet watched based on the movie's features. This is why this approach is referred to as item-centred. In other words, if we have enough data, we train a model for each user based on the previously watched movies and their features. The features we have chosen to use in our model are given genres, vote average, release date and runtime. While release date and vote average are available, we have to extract and process the genres for each movie. Given that we have a list of genres for each movie, we will have to dummy encode it as follows:\n",
    "- Determine how many genres there are and make each genre a feature of the model\n",
    "- Assign to each genre a zero if it is not present in the movie's list of genres, or 1 for each genre given each movie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our data\n",
    "links_small = pd.read_csv(\"../DATA/links_small.csv\")\n",
    "ratings_small = pd.read_csv(\"../DATA/ratings_small.csv\")\n",
    "md = pd.read_csv(\"../DATA/movies_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying our data\n",
    "# md\n",
    "# ratings_small\n",
    "#links_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only considering movies with the upper 15% most votes\n",
    "m = md[\"vote_count\"].quantile(0.85)\n",
    "md = md[md[\"vote_count\"] > m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure imdb_id matches between links and metadata\n",
    "md[\"imdb_id\"] = md[\"imdb_id\"].str.strip(\"tt\")\n",
    "\n",
    "# Removing all movies without a genre, release_date, imdb_id or vote_average\n",
    "md[\"genres\"].replace('[]', np.nan, inplace=True)\n",
    "md.dropna(subset=[\"genres\", \"release_date\", \"imdb_id\", \"vote_average\"], inplace=True)\n",
    "\n",
    "# Converting release_date to a POSIX timestamp float\n",
    "md[\"release_date\"] = pd.to_datetime(md[\"release_date\"], infer_datetime_format=True)\n",
    "md[\"release_date\"] = md[\"release_date\"].apply(lambda x:x.timestamp())\n",
    "\n",
    "# Converting imdb_id to int\n",
    "md[\"imdb_id\"] = md[\"imdb_id\"].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting \"genres\" column from a dictionary from a list of strings, containing the respective genres\n",
    "def extract_genres(x):\n",
    "    genre_string = ''\n",
    "    x = eval(x) # executes expression inside of string\n",
    "    for dictionary in x:\n",
    "        genre_string += dictionary[\"name\"] + '|'\n",
    "    return genre_string # include all but last one\n",
    "md[\"genres\"] = md[\"genres\"].apply(extract_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the data we are going to use for this model, we can extract the three columns: genre, release_data and budget. We will now also determine the unique features present in the dataset and use each as a feature. Note that this model assumes that all possible genres are included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising our features matrix\n",
    "FEATURES = md[[\"imdb_id\", \"original_title\", \"release_date\", \"vote_average\"]]\n",
    "\n",
    "# Finding unique genre names\n",
    "GENRES = md[\"genres\"]\n",
    "unique_genres = list(set(GENRES.sum().split('|')[:-1])) # Don't include last element\n",
    "print(unique_genres)\n",
    "\n",
    "# Removing '|' from the end of each string\n",
    "GENRES = GENRES.apply(lambda x:x[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding each genre as a feature\n",
    "extended_features = GENRES.str.get_dummies()\n",
    "\n",
    "# Horizontally stack our extended features and the original features\n",
    "FEATURES = FEATURES.merge(extended_features, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Normalizing non-categorical features\n",
    "scaler = MinMaxScaler()\n",
    "FEATURES[['release_date', 'vote_average']] = scaler.fit_transform(FEATURES[['release_date', 'vote_average']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our features for each movie are set, we need to create a list of users, where each user contains the features and ratings of each of the movies they rated. To link the users and their ratings to the movies, we will need to use the intermediary \"links\" table. Be careful! We have dropped a few of the movies in the original dataset when the required feature was not available. First we will merge the appropriate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Join: links JOIN FEATURES ON imdbId\n",
    "FEATURES = FEATURES.rename(columns={'imdb_id':'imdbId'}) # making column names match\n",
    "first_join = links_small.merge(FEATURES, on=\"imdbId\")\n",
    "\n",
    "# Second Join: ratings JOIN first_join on movieId\n",
    "data_matrix = ratings_small.merge(first_join, on=\"movieId\")\n",
    "\n",
    "# Delete unnecessary columns\n",
    "data_matrix.drop(['movieId', 'timestamp', 'imdbId', 'tmdbId'],axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging our dataframes, we will create a list of users, each with all the ratings provided by each user and features of the corresponding rated movie. But how do we evaluate our errors? A naive approach would be to compute the chosen metric(s) for each user, then average their values over all users. But is that a fair measure? What if one user gave us a test set size of 70 and another a test set 8? Should the results of these be weighted equally?\n",
    "\n",
    "The answer is no. To account for this imbalance, we would have to remedy this by taking a weighted average given the number of movies rated for a given user. To make things simpler instead of having to compute this weighted average, we will have global trackers of labels and predictions over all users, then apply our metrics over that. This way, our user that contributed to 70 ratings for the test set will have a larger contribution than the user with a test set size of 8 ratings by default! This will become clearer after a closer inspection of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unique user list\n",
    "unique_users = list(set(data_matrix[\"userId\"]))\n",
    "user_data = {}\n",
    "\n",
    "# Adding movie ratings and features to the list of the corresponding user for users that have rated more than tr movies\n",
    "data_copy = data_matrix.copy()\n",
    "tr = 50 # at least n reviews\n",
    "for user_id in unique_users:\n",
    "    current_data = data_copy[data_copy[\"userId\"] == user_id]\n",
    "    if current_data.shape[0] >= tr:\n",
    "        user_data[user_id] = current_data\n",
    "    data_copy = data_copy[data_copy[\"userId\"] != user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Train regression model on each user!\n",
    "user_models = {}\n",
    "scores = []\n",
    "labels = []\n",
    "total_predictions = []\n",
    "for user_id in tqdm(user_data.keys()):\n",
    "    # Get user data\n",
    "    data = user_data[user_id]\n",
    "    Y = data[\"rating\"]\n",
    "    X = data.drop([\"userId\", \"rating\", \"original_title\"], axis=1)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    # Train model and predict on test data\n",
    "    reg = RandomForestRegressor(n_estimators=40, max_depth=3, random_state=0)\n",
    "    fitted_reg = reg.fit(X_train, Y_train)\n",
    "    predictions = fitted_reg.predict(X_test)\n",
    "    user_models[user_id] = fitted_reg # storing estimator for this user!\n",
    "    labels.extend(Y_test)\n",
    "    total_predictions.extend(predictions)\n",
    "    \n",
    "    # Optional: compute cross validated metrics in your own time (may take some time!)\n",
    "#     scores.append(cross_validate(reg, X, Y, cv=5, scoring=('neg_mean_absolute_error', 'neg_root_mean_squared_error')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying metrics that measure our models performance\n",
    "print(\"MAE:\", MAE(labels, total_predictions))\n",
    "print(\"RMSE:\", MSE(labels, total_predictions, squared=False))\n",
    "\n",
    "# Displaying metrics for 5-fold cross validation\n",
    "# print(\"5-Fold MAE: \", abs(np.mean([score[\"test_neg_mean_absolute_error\"] for score in scores])))\n",
    "# print(\"5-fold RMSE: \", abs(np.mean([score[\"test_neg_root_mean_squared_error\"] for score in scores])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are relatively okay. The MAE is the more easily interpretable metric and tells us that, on average, we deviate from the true rating by about 0.64. The RMSE metric adds more weights to large errors because of how it is calculated, and highlights large errors that may arise by how much larger it is than the RMSE.\n",
    "\n",
    "Now that we have trained a Random Forest Regressor model on every user, we can now pick any random user and recommend them the movies that receive the highest ratings according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "user_id = random.choice(list(user_data.keys()))\n",
    "\n",
    "# Check the predictions of the recommender\n",
    "model = user_models[user_id]\n",
    "movies = FEATURES.drop(axis=1, columns=[\"imdbId\"]) # We don't care about the imdbId\n",
    "# We want the title, but not as our model input\n",
    "movies[\"rating\"] = model.predict(movies.drop(axis=1, columns=[\"original_title\"])) \n",
    "\n",
    "# Diplay top 10 rated and top 10 recommendations!\n",
    "top_rated = user_data[user_id].nlargest(columns=\"rating\", n=10)\n",
    "top_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 recommendations\n",
    "movies.nlargest(columns=\"rating\", n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the evaluation metrics, we can see that the prediction made by our model deviates on average by 0.63 from the true rating, which is not a large deviation, and is generally able to distinguish between 'okay', 'great' and 'terrible' movies in the opinion of the user. However, if we would like to recommend 10 movies out of roughly 45,000, our rating predictions need to be even better. Therefore, this model may serve as a strong baseline model that can be improved by accounting for other features such as cast, directors, plot, amongst others.\n",
    "\n",
    "### Limitations:\n",
    "As you have seen, we only considered users that had reviewed 100 movies or more, as we do not have enough data to accurately train a model for that given user for a large number of features. What happens in the real-world if we need to make recommendations for users who have rated few movies? What if they are a completely new user? This is known as the __cold-start__ problem and makes it so that models such as our content-based one struggle to make recommendations for users whom we have little data on, making it an issue of __data sparsity__. Additionally, this approach requires feature extraction, which can sometimes be cumbersome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative-filtering\n",
    "\n",
    "The second classification of algorithms we are now going to go over is known as __collaborative-filtering__, which, as the name implies, requires the collaboration of all the different members of our user-base. It works under the assumption that users that have had similar preferences/choices in the past have similar tastes. If Susan and Tom both love the same movies, if Susan loves _He's Just Not That Into You_ then we should probably recommend that movie to Tom! Unlike content-based recommender systems, collaborative filtering does not require any manual feature extraction, hence why it's referred to as user-centred.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/676/1*_x4VqIeV9L6fxXm5PeAYsg.png\" width=\"500px\" height=\"500px\">\n",
    "\n",
    "[Source](https://medium.com/sfu-cspmp/recommendation-systems-user-based-collaborative-filtering-using-n-nearest-neighbors-bf7361dc24e0)\n",
    "\n",
    "As before, there are different approaches to how we can create a collaborative filtering system:\n",
    "- We can either look at the preferences/ratings of users who have similar interests and predict the preferences/ratings of this user (memory-based)\n",
    "- We can use the data we have on the preferences/ratings of all users in our dataset to __learn__ and/or __predict__ the features of individual different items and use these features to predict preferences/ratings for given items (model-based)\n",
    "\n",
    "As we have covered the model-based approach in the content-based recommendation section, we will be implementing a memory-based approach for collaborative filtering by using some of the unsupervised learning techniques we have encountered so far in the course. If you are interested in looking into an implementation that uses feature learning, check out this [video](https://www.youtube.com/watch?v=9AP-DgFBNP4&ab_channel=ArtificialIntelligence-AllinOne).\n",
    "\n",
    "More specifically, we will be creating a ratings vector for each user and using a KNN with means algorithm to compute mean rating of the user's $K$ nearest neighbours for movies not yet rated by a given user. We will be using the __Surprise library__, a scikit for building and analyzing recommender systems that deal with explicit rating data. Before we use our data we will have to clean it just as in the case for the content-based system. For the Surprise 'cross-validate' method, we need to have our data as three columns in the following order: userId, movieId, rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing our data\n",
    "links_small = pd.read_csv(\"../DATA/links_small.csv\")\n",
    "ratings_small = pd.read_csv(\"../DATA/ratings_small.csv\")\n",
    "md = pd.read_csv(\"../DATA/movies_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering movie data by vote count\n",
    "md = md[md[\"vote_count\"] >= 50]\n",
    "md.dropna(subset=[\"imdb_id\", \"title\"], inplace=True)\n",
    "md.drop_duplicates(subset=[\"imdb_id\"], inplace=True)\n",
    "md[\"imdb_id\"] = md[\"imdb_id\"].str.strip(\"tt\").astype('int64')\n",
    "\n",
    "# Merging our data\n",
    "movies = md[[\"imdb_id\", \"title\"]]\n",
    "movies.rename(columns={'imdb_id':'imdbId'}, inplace=True) # making column names match\n",
    "first_join = links_small.merge(movies, on=\"imdbId\")\n",
    "joined_data = ratings_small.merge(first_join, on=\"movieId\")\n",
    "joined_data.drop(['title', 'timestamp', 'tmdbId', 'imdbId'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.knns import  KNNWithMeans\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "data = Dataset.load_from_df(joined_data, reader)\n",
    "algo = KNNWithMeans()\n",
    "cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we can see that they carried out the KNNWithMeans and got relatively good scores from the baseline model! But how does it work? Let us look at what the Surprise library is actually doing. What does the input data look like? What does their predictions look like and how is their model evaluated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What does the data look like?\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "# testset\n",
    "\n",
    "# What do the predictions look like?\n",
    "# algo = KNNWithMeans().fit(trainset)\n",
    "# predictions = algo.test(testset)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this algorithm actually doing? In memory-based approaches with algorithms such as KNN, we generally construct a __user-item matrix,__ as seen in the diagram above. This is a 2D representation of users vs items, such that each row contains all the ratings (and missing ratings) a user gave to every movie. Cells representing movies that have not been rated by the respective users are filled with zeros, NaNs or other placeholders. In the case of this surprise algorithm, they created a __similarity matrix__ containing the measure of cosine similarity between each user pair and uses that similarity as weights when computing the average rating of all its neighbours.\n",
    "\n",
    "Some of the most popular frameworks for collaborative filtering algorithms use __matrix factorization__ approaches, which include using a variant of SVD (as SVD cannot be applied to a matrix with missing data), known as SVD++. In fact, this is the algorithm popularized by Simon Funk during the Netflix prize for recommender systems! The theory of this algorithm extends beyond the scope of this course, but you can read more about it [here](https://www.hindawi.com/journals/mpe/2017/1975719/). Luckily this algorithm is also implemented by the Surprise library!\n",
    "\n",
    "[Documentation](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "# CF Algorithm using SVD++\n",
    "algo = SVD()\n",
    "cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "Just like with the content-based recommender system, a collaborative filtering system also suffers from a cold start problem. In fact, since it is solely based on the ratings/preferences and not on any metadata, it requires much more data. It can also sometimes be difficult to combine and weight the preferences of user neighbors. \n",
    "\n",
    "In the real-world, you will rarely see a model that is purely content-based or purely collaborative filtering. Generally, multiple models that capture the predicted preferences differently are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Systems\n",
    "\n",
    "As the name implies __hybrid systems__ are essentially ensemble models of different recommender systems together. As each approach generally focuses on a different characteristic and suffers from individual limitations, hybrid systems are generally able to give the optimal result. Arguably one of the most successful recommender systems, the one used by _Spotify_, works with this principle. It combines the predictions of the following three key systems:\n",
    "\n",
    "1. Collaborative filtering to compare an individual's behaviour to other people's taste\n",
    "2. Natural language processing to analyse the text in each song and find songs of similar lyrics (content-based)\n",
    "3. Audio modelling of songs' raw audio (content-based)\n",
    "\n",
    "Unlike other recommendation systems, Spotify uses __implicit feedback__ instead of ratings, which are things such as stream count. This first model gives their system a way to recommend songs based on similar users. The NLP and audio modelling content-based models enable the system to find similarities between items with very few streams which would not otherwise be recommended by the collaborative filtering model due to data sparsity. This is how Spotify is able to recommend even relatively unpopular songs based on a user's personal preferences, which is what made their 'Discover Weekly' feature so popular!\n",
    "\n",
    "If you would like to practice developing your own recommender systems, a great place to find datasets that can be used for recommender systems is [here](https://github.com/caserec/Datasets-for-Recommender-Systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
